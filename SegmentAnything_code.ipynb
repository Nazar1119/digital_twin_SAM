{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading image and load SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_original = cv2.imread('img1.jpg')\n",
    "image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image = cv2.GaussianBlur(image_original, (5, 5), 0)\n",
    "clahe = cv2.createCLAHE(clipLimit=4, tileGridSize=(16, 16))\n",
    "image = np.stack([clahe.apply(image[:, :, i]) for i in range(3)], axis=2)\n",
    "image_resized = cv2.resize(image, (1024, 768))\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function \"filter_sinter_stones\" filters a list of masks (generated by SAM) to keep only those that meet specific criteria, ensuring that the resulting masks represent meaningful agglomerate stones rather than noise, small fragments, or redundant overlapping regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sinter_stones(masks, area_range=(100, 5000), min_aspect_ratio=0.5, min_iou_with_largest=0.3):\n",
    "    filtered_masks = []\n",
    "    if not masks:\n",
    "        return filtered_masks\n",
    "    \n",
    "    sorted_masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "    \n",
    "    for i, mask in enumerate(sorted_masks):\n",
    "        area = mask['area']\n",
    "        bbox = mask['bbox']\n",
    "        width = bbox[2]\n",
    "        height = bbox[3]\n",
    "        aspect_ratio = min(width / height, height / width)\n",
    "        \n",
    "        is_redundant = False\n",
    "        for larger_mask in sorted_masks[:i]:\n",
    "            larger_seg = larger_mask['segmentation']\n",
    "            current_seg = mask['segmentation']\n",
    "            intersection = np.logical_and(larger_seg, current_seg).sum()\n",
    "            union = np.logical_or(larger_seg, current_seg).sum()\n",
    "            iou = intersection / union if union > 0 else 0\n",
    "            if iou > min_iou_with_largest:\n",
    "                is_redundant = True\n",
    "                break\n",
    "        \n",
    "        if (area_range[0] <= area <= area_range[1] and \n",
    "            aspect_ratio >= min_aspect_ratio and \n",
    "            not is_redundant):\n",
    "            filtered_masks.append(mask)\n",
    "    \n",
    "    return filtered_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function \"merge_overlapping_masks\" takes a list of masks and merges masks that overlap significantly into a single mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_masks(masks, iou_threshold):\n",
    "    if not masks:\n",
    "        return masks\n",
    "    \n",
    "    merged_masks = []\n",
    "    remaining_masks = masks.copy()\n",
    "    \n",
    "    while remaining_masks:\n",
    "        current_mask = remaining_masks.pop(0)\n",
    "        current_seg = current_mask['segmentation']\n",
    "        current_bbox = current_mask['bbox']\n",
    "        \n",
    "        overlapping = []\n",
    "        for i, other_mask in enumerate(remaining_masks):\n",
    "            other_seg = other_mask['segmentation']\n",
    "            intersection = np.logical_and(current_seg, other_seg).sum()\n",
    "            union = np.logical_or(current_seg, other_seg).sum()\n",
    "            iou = intersection / union if union > 0 else 0\n",
    "            \n",
    "            if iou > iou_threshold:\n",
    "                overlapping.append(i)\n",
    "        \n",
    "        for idx in sorted(overlapping, reverse=True):\n",
    "            other_mask = remaining_masks.pop(idx)\n",
    "            current_seg = np.logical_or(current_seg, other_mask['segmentation'])\n",
    "            other_bbox = other_mask['bbox']\n",
    "            current_bbox = [\n",
    "                min(current_bbox[0], other_bbox[0]),\n",
    "                min(current_bbox[1], other_bbox[1]),\n",
    "                max(current_bbox[0] + current_bbox[2], other_bbox[0] + other_bbox[2]) - min(current_bbox[0], other_bbox[0]),\n",
    "                max(current_bbox[1] + current_bbox[3], other_bbox[1] + other_bbox[3]) - min(current_bbox[1], other_bbox[1])\n",
    "            ]\n",
    "        \n",
    "        current_mask['segmentation'] = current_seg\n",
    "        current_mask['bbox'] = current_bbox\n",
    "        merged_masks.append(current_mask)\n",
    "    \n",
    "    return merged_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function \"exclude_text_regions\" its for excluding a region with text at image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_text_regions(masks, image_shape, text_regions):\n",
    "    height, width = image_shape[:2]\n",
    "    filtered_masks = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        m = mask['segmentation']\n",
    "        mask_height, mask_width = m.shape\n",
    "        overlaps_text = False\n",
    "        \n",
    "        for x, y, w, h in text_regions:\n",
    "            mask_x, mask_y = int(x * mask_width / width), int(y * mask_height / height)\n",
    "            mask_w, mask_h = int(w * mask_width / width), int(h * mask_height / height)\n",
    "            \n",
    "            if np.any(m[max(0, mask_y):min(mask_height, mask_y + mask_h),\n",
    "                        max(0, mask_x):min(mask_width, mask_x + mask_w)]):\n",
    "                overlaps_text = True\n",
    "                break\n",
    "        \n",
    "        if not overlaps_text:\n",
    "            filtered_masks.append(mask)\n",
    "    \n",
    "    return filtered_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_segmentation(masks, base_image):\n",
    "    segmented_image = base_image.copy()\n",
    "    colors = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 1, 1], [1, 0, 1]]\n",
    "    \n",
    "    for i, mask in enumerate(masks):\n",
    "        m = mask['segmentation']\n",
    "        m_resized = cv2.resize(m.astype(np.uint8), (segmented_image.shape[1], segmented_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        color = colors[i % len(colors)]\n",
    "        color_mask = np.zeros_like(segmented_image, dtype=np.float32)\n",
    "        \n",
    "        for c in range(3):\n",
    "            color_mask[:, :, c] = m_resized * color[c]\n",
    "        color_mask = np.uint8(color_mask * 0.3 * 255)\n",
    "        segmented_image = cv2.addWeighted(segmented_image, 1, color_mask, 0.7, 0)\n",
    "    \n",
    "    return segmented_image\n",
    "\n",
    "def draw_bounding_boxes(image, masks, color=(0, 255, 0), thickness=1):\n",
    "    result_image = image.copy()\n",
    "    processed_bboxes = set()\n",
    "    \n",
    "    for mask in masks:\n",
    "        bbox = mask['bbox']\n",
    "        bbox_tuple = (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]))\n",
    "        if bbox_tuple not in processed_bboxes:\n",
    "            x_min, y_min, w, h = bbox_tuple\n",
    "            cv2.rectangle(result_image, (x_min, y_min), (x_min + w, y_min + h), color, thickness)\n",
    "            processed_bboxes.add(bbox_tuple)\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask generation with SAM and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks generated: 662\n"
     ]
    }
   ],
   "source": [
    "mask_generator1_ = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=64,\n",
    "    pred_iou_thresh=0.7,\n",
    "    stability_score_thresh=0.7,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=50,  # Increased from 1 to reduce small masks\n",
    ")\n",
    "\n",
    "masks_original = mask_generator1_.generate(image_resized)\n",
    "print(f\"Number of masks generated: {len(masks_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_regions = [(0, 0, 300, 50), (image_resized.shape[1] - 300, image_resized.shape[0] - 70, 300, 50)]\n",
    "\n",
    "filtered_masks = exclude_text_regions(masks_original, image_resized.shape, text_regions)\n",
    "sinter_stone_masks = filter_sinter_stones(filtered_masks, area_range=(100, 5000), min_aspect_ratio=0.5, min_iou_with_largest=0.3)\n",
    "sinter_stone_masks = merge_overlapping_masks(sinter_stone_masks, iou_threshold=0.7)\n",
    "\n",
    "# Generate segmented image and add bounding boxes\n",
    "segmented_image = color_segmentation(sinter_stone_masks, image_resized)\n",
    "segmented_image_with_boxes = draw_bounding_boxes(segmented_image, sinter_stone_masks, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# Display and save results\n",
    "plt.figure(figsize=(50, 25))\n",
    "plt.imshow(segmented_image_with_boxes)\n",
    "plt.axis('off')\n",
    "plt.savefig('segmented_image_with_one_box_per_stone6.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
